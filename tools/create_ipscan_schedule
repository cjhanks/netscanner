#!/usr/bin/env python3

import argparse
import boto3
import multiprocessing as mp
import os
import ipaddress
import json
import re
import sys
import yaml

class Writer:
    mapping = [
        '|',
        '/',
        '-',
        '\\',
    ]
    def __init__(self):
        self.length = 0
        self.n = 0

    def write(self, msg):
        msg += ' '
        msg += self.mapping[self.n]
        sys.stderr.write('\b' * self.length)
        sys.stderr.write(msg)
        self.length = len(msg)
        sys.stderr.flush()

        self.n += 1
        self.n %= len(self.mapping)

def load_data(args):
    (name, root) = args
    path = os.path.join(root, 'prefixes.yml')
    with open(path, 'r') as fp:
        data = yaml.load(fp, Loader=yaml.CLoader)
        if 'options' in data:
            del data['options']

    cidr_output = []
    cidr_blocks = list(data.keys())

    for cidr in cidr_blocks:
        m = re.match('([0-9\.]+)/([0-9]+)', cidr)
        if not m:
            continue

        ipv4 = ipaddress.IPv4Network(cidr)
        if ipv4.prefixlen < 22:
            for subnet in ipv4.subnets(new_prefix=24):
                cidr_output.append(str(subnet))
        else:
            cidr_output.append(cidr)

    return {name: cidr_output}

def main(argv=None):
    writer = Writer()
    argp = argparse.ArgumentParser()
    argp.add_argument(
        '-r',
        '--root',
    )
    argp.add_argument(
        '-q',
        '--queue',
        default='QueueProcessCIDR',
    )
    args = argp.parse_args(argv)
    asns = [(p, os.path.join(args.root, p)) for p in os.listdir(args.root)]

    # -
    pool = mp.Pool(4)
    data = {}

    print()
    print('Load CIDR Blocks')
    n = 0
    for resp in pool.imap_unordered(load_data, asns):
        data.update(resp)

        if n % 100 == 0:
            writer.write('%5.2f' % (n / len(asns)))
        n += 1

    # -
    # Count all of the IP blocks
    total_jobs = 0
    for (asn, cidrs) in data.items():
        total_jobs += len(cidrs)

    maxmum_jobs = total_jobs
    total_jobs = min(total_jobs, 150000)

    # -
    all_jobs = []

    for n in range(total_jobs):
        job = {
            's3bucket': 'netscanner',
            's3output': '2024-02-27',
            'scanopts': [
                '--open',
                '-sS',
                '-p',
                    '20,21,22,23,80,443,1194,5000,5001,8000,8080,8443,9001,9030'
            ],
            'jobs'    : []
        }
        all_jobs.append(job)

    #
    print()
    print('Create Schedule')
    n = 0
    for (asn, cidrs) in data.items():
        if n % 100 == 0:
            writer.write('%5.2f' % (n / len(data)))
        n += 1

        for cidr in cidrs:
            job = {
                'asn' : asn,
                'cidr': cidr
            }

            all_jobs[n % total_jobs]['jobs'].append(job)


    print(maxmum_jobs)
    print(len(all_jobs))
    print(len(json.dumps(all_jobs[0])) / 1024.0)

    # -
    # Schedule
    client = boto3.resource('sqs', region_name='us-west-1')
    queue = client.get_queue_by_name(QueueName=args.queue)

    entries = []
    print()
    print('Schedule Jobs')
    for (i, job) in enumerate(all_jobs):
        if i % 100 == 0:
            writer.write('%5.2f' % (i / len(all_jobs)))

        entries.append({
            'Id': '%06d' % i,
            'MessageBody': json.dumps(job)
        })

        if len(entries) % 10 == 0:
            queue.send_messages(Entries=entries)
            entries = []

    if entries:
        queue.send_messages(Entries=entries)


if __name__ == '__main__':
    main([
        '-r',
            #'./demo'
            './data/ASN'
    ])
